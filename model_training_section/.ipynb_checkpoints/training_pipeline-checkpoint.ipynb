{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d68439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2ae020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\"\"\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9403e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_dir = glob.glob('./../../dataset/*.jpg')\n",
    "train_set = set_dir[:5703]\n",
    "val_set = set_dir[5703:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40633dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIC_SIZE = 256\n",
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.transforms = transforms.Resize((PIC_SIZE, PIC_SIZE),  Image.BICUBIC)\n",
    "        \n",
    "        self.pic_size = PIC_SIZE\n",
    "        self.paths = paths\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        img = self.transforms(img)\n",
    "        img = np.array(img)\n",
    "        img_lab = rgb2lab(img).astype('float32')\n",
    "        img_lab = transforms.ToTensor()(img_lab)\n",
    "        L = img_lab[[0], ...] / 50. -1.\n",
    "        ab = img_lab[[1, 2], ...] / 110.\n",
    "               \n",
    "        return {'L': L, 'ab':ab}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b59ae1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_dataloaders(batch_size=16, n_workers=2, pin_memory=True, **kwargs):\n",
    "    dataset = ColorizationDataset(**kwargs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,pin_memory=pin_memory)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "train_dl = make_dataloaders(paths=train_set)\n",
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98d4077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = relu(self.conv1(inputs))\n",
    "        x = relu(self.conv2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4894ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.convolution = Conv_block(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "    def forward (self, inputs):\n",
    "        x = self.convolution(inputs)\n",
    "        p = self.pool(x)\n",
    "        \n",
    "        return x, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2de5eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_block(nn.Module):\n",
    "    def __init__ (self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2)\n",
    "        self.dconv1 = nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.dconv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.upconv(inputs)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = relu(self.dconv1(x))\n",
    "        x = relu(self.dconv2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d8b266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #Encoder part\n",
    "        self.econv1 = Encoder_block(1, 64)\n",
    "        self.econv2 = Encoder_block(64, 128)\n",
    "        self.econv3 = Encoder_block(128, 256)\n",
    "        self.econv4 = Encoder_block(256, 512)\n",
    "        \n",
    "        #Bootleneck\n",
    "        self.b = Conv_block(512, 1024)\n",
    "        \n",
    "        #Decoder part\n",
    "        self.dconv1 = Decoder_block(1024, 512)\n",
    "        self.dconv2 = Decoder_block(512,256)\n",
    "        self.dconv3 = Decoder_block(256, 128)\n",
    "        self.dconv4 = Decoder_block(128, 64)\n",
    "        \n",
    "        #Output layer\n",
    "        self.outconv = nn.Conv2d(64, 2, kernel_size = 1)\n",
    "        \n",
    "    def forward (self, inputs):\n",
    "        #Encoder part\n",
    "        x1, p1 = self.econv1(inputs)\n",
    "        x2, p2 = self.econv2(p1)       \n",
    "        x3, p3 = self.econv3(p2)        \n",
    "        x4, p4 = self.econv4(p3)\n",
    "        \n",
    "        #Bootleneck\n",
    "        b = self.b(p4)\n",
    "        \n",
    "        #Decoder part\n",
    "        d1 = self.dconv1(b, x4)\n",
    "        d2 = self.dconv2(d1, x3)\n",
    "        d3 = self.dconv3(d2, x2)\n",
    "        d4 = self.dconv4(d3, x1)\n",
    "        \n",
    "        #Output layer\n",
    "        out = self.outconv(d4)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = self.forward(input_data)\n",
    "            \n",
    "        prediction = torch.cat ((input_data, pred), dim=1)\n",
    "        prediction[:, 0, :, :] += 1.\n",
    "        prediction[:, 0, :, :] *= 50.\n",
    "        prediction[:, 1, :, :] *= 110.\n",
    "        prediction[:, 2, :, :] *= 110.\n",
    "        pred_arr = prediction.numpy()\n",
    "        \n",
    "        from skimage import color\n",
    "        rgb_image = np.transpose(pred_arr, (0, 2, 3, 1))\n",
    "        rgb_image = color.lab2rgb(rgb_image[0])\n",
    "        rgb_image = np.clip(rgb_image, 0, 1) * 255\n",
    "        rgb_image = rgb_image.astype(np.uint8)\n",
    "        image_pil = Image.fromarray(rgb_image)\n",
    "            \n",
    "        self.train()\n",
    "        return image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a033fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/357 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0002\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dl, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mab\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m (\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m#Encoder part\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     x1, p1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     x2, p2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meconv2(p1)       \n\u001b[1;32m     26\u001b[0m     x3, p3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meconv3(p2)        \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mEncoder_block.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m (\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m      8\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvolution(inputs)\n\u001b[0;32m----> 9\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, p\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_dl, epochs, learning_rate=0.001):\n",
    "    criterion = nn.MSELoss()  # Funkcja straty Mean Squared Error\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_idx, data in tqdm(enumerate(train_dl), total=len(train_dl)):\n",
    "            inputs, targets = data['L'], data['ab']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dl)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "model = UNet().to('cpu')\n",
    "train_model(model, train_dl, epochs=100, learning_rate=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a530dbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet()\n",
    "model.load_state_dict(torch.load('./trained_model.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "643054f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]             640\n",
      "            Conv2d-2         [-1, 64, 256, 256]          36,928\n",
      "        Conv_block-3         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-4         [-1, 64, 128, 128]               0\n",
      "     Encoder_block-5  [[-1, 64, 256, 256], [-1, 64, 128, 128]]               0\n",
      "            Conv2d-6        [-1, 128, 128, 128]          73,856\n",
      "            Conv2d-7        [-1, 128, 128, 128]         147,584\n",
      "        Conv_block-8        [-1, 128, 128, 128]               0\n",
      "         MaxPool2d-9          [-1, 128, 64, 64]               0\n",
      "    Encoder_block-10  [[-1, 128, 128, 128], [-1, 128, 64, 64]]               0\n",
      "           Conv2d-11          [-1, 256, 64, 64]         295,168\n",
      "           Conv2d-12          [-1, 256, 64, 64]         590,080\n",
      "       Conv_block-13          [-1, 256, 64, 64]               0\n",
      "        MaxPool2d-14          [-1, 256, 32, 32]               0\n",
      "    Encoder_block-15  [[-1, 256, 64, 64], [-1, 256, 32, 32]]               0\n",
      "           Conv2d-16          [-1, 512, 32, 32]       1,180,160\n",
      "           Conv2d-17          [-1, 512, 32, 32]       2,359,808\n",
      "       Conv_block-18          [-1, 512, 32, 32]               0\n",
      "        MaxPool2d-19          [-1, 512, 16, 16]               0\n",
      "    Encoder_block-20  [[-1, 512, 32, 32], [-1, 512, 16, 16]]               0\n",
      "           Conv2d-21         [-1, 1024, 16, 16]       4,719,616\n",
      "           Conv2d-22         [-1, 1024, 16, 16]       9,438,208\n",
      "       Conv_block-23         [-1, 1024, 16, 16]               0\n",
      "  ConvTranspose2d-24         [-1, 1024, 32, 32]       4,195,328\n",
      "           Conv2d-25          [-1, 512, 32, 32]       7,078,400\n",
      "           Conv2d-26          [-1, 512, 32, 32]       2,359,808\n",
      "    Decoder_block-27          [-1, 512, 32, 32]               0\n",
      "  ConvTranspose2d-28          [-1, 512, 64, 64]       1,049,088\n",
      "           Conv2d-29          [-1, 256, 64, 64]       1,769,728\n",
      "           Conv2d-30          [-1, 256, 64, 64]         590,080\n",
      "    Decoder_block-31          [-1, 256, 64, 64]               0\n",
      "  ConvTranspose2d-32        [-1, 256, 128, 128]         262,400\n",
      "           Conv2d-33        [-1, 128, 128, 128]         442,496\n",
      "           Conv2d-34        [-1, 128, 128, 128]         147,584\n",
      "    Decoder_block-35        [-1, 128, 128, 128]               0\n",
      "  ConvTranspose2d-36        [-1, 128, 256, 256]          65,664\n",
      "           Conv2d-37         [-1, 64, 256, 256]         110,656\n",
      "           Conv2d-38         [-1, 64, 256, 256]          36,928\n",
      "    Decoder_block-39         [-1, 64, 256, 256]               0\n",
      "           Conv2d-40          [-1, 2, 256, 256]             130\n",
      "================================================================\n",
      "Total params: 36,950,338\n",
      "Trainable params: 36,950,338\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 44563978.00\n",
      "Params size (MB): 140.95\n",
      "Estimated Total Size (MB): 44564119.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1011ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_check(validiation_set):\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    from tqdm.notebook import tqdm\n",
    "    ssim_list = list()\n",
    "    for elements in tqdm(val_set):\n",
    "        pic = glob.glob(elements)\n",
    "        img_test = make_dataloaders(paths=pic, batch_size=1)\n",
    "        channel = next(iter(img_test))\n",
    "\n",
    "        Ls, abs_ = channel['L'], channel['ab']\n",
    "        \n",
    "        pred = model.predict(Ls)\n",
    "    \n",
    "        image_ref = Image.open(elements)\n",
    "        image_ref = image_ref.resize((256, 256), Image.BICUBIC)\n",
    "        image_ref = np.array(image_ref)\n",
    "        \n",
    "        image_pred = np.array(pred)\n",
    "        \n",
    "        ssim_val = ssim(image_ref, image_pred, win_size=3, multichannel=True)\n",
    "        ssim_list.append(ssim_val)       \n",
    "        \n",
    "    return ssim_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
