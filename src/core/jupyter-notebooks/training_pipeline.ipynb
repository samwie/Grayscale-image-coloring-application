{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d68439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2ae020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\"\"\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9403e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_dir = glob.glob('./../../dataset/*.jpg')\n",
    "train_set = set_dir[:5703]\n",
    "val_set = set_dir[5703:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40633dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIC_SIZE = 256\n",
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.transforms = transforms.Resize((PIC_SIZE, PIC_SIZE),  Image.BICUBIC)\n",
    "        \n",
    "        self.pic_size = PIC_SIZE\n",
    "        self.paths = paths\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        img = self.transforms(img)\n",
    "        img = np.array(img)\n",
    "        img_lab = rgb2lab(img).astype('float32')\n",
    "        img_lab = transforms.ToTensor()(img_lab)\n",
    "        L = img_lab[[0], ...] / 50. -1.\n",
    "        ab = img_lab[[1, 2], ...] / 110.\n",
    "               \n",
    "        return {'L': L, 'ab':ab}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b59ae1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_dataloaders(batch_size=16, n_workers=2, pin_memory=True, **kwargs):\n",
    "    dataset = ColorizationDataset(**kwargs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,pin_memory=pin_memory)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "train_dl = make_dataloaders(paths=train_set)\n",
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98d4077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = relu(self.conv1(inputs))\n",
    "        x = relu(self.conv2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4894ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.convolution = Conv_block(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "    def forward (self, inputs):\n",
    "        x = self.convolution(inputs)\n",
    "        p = self.pool(x)\n",
    "        \n",
    "        return x, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2de5eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_block(nn.Module):\n",
    "    def __init__ (self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2)\n",
    "        self.dconv1 = nn.Conv2d(in_channels+out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.dconv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.upconv(inputs)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = relu(self.dconv1(x))\n",
    "        x = relu(self.dconv2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d8b266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #Encoder part\n",
    "        self.econv1 = Encoder_block(1, 64)\n",
    "        self.econv2 = Encoder_block(64, 128)\n",
    "        self.econv3 = Encoder_block(128, 256)\n",
    "        self.econv4 = Encoder_block(256, 512)\n",
    "        \n",
    "        #Bootleneck\n",
    "        self.b = Conv_block(512, 1024)\n",
    "        \n",
    "        #Decoder part\n",
    "        self.dconv1 = Decoder_block(1024, 512)\n",
    "        self.dconv2 = Decoder_block(512,256)\n",
    "        self.dconv3 = Decoder_block(256, 128)\n",
    "        self.dconv4 = Decoder_block(128, 64)\n",
    "        \n",
    "        #Output layer\n",
    "        self.outconv = nn.Conv2d(64, 2, kernel_size = 1)\n",
    "        \n",
    "    def forward (self, inputs):\n",
    "        #Encoder part\n",
    "        x1, p1 = self.econv1(inputs)\n",
    "        x2, p2 = self.econv2(p1)       \n",
    "        x3, p3 = self.econv3(p2)        \n",
    "        x4, p4 = self.econv4(p3)\n",
    "        \n",
    "        #Bootleneck\n",
    "        b = self.b(p4)\n",
    "        \n",
    "        #Decoder part\n",
    "        d1 = self.dconv1(b, x4)\n",
    "        d2 = self.dconv2(d1, x3)\n",
    "        d3 = self.dconv3(d2, x2)\n",
    "        d4 = self.dconv4(d3, x1)\n",
    "        \n",
    "        #Output layer\n",
    "        out = self.outconv(d4)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = self.forward(input_data)\n",
    "            \n",
    "        prediction = torch.cat ((input_data, pred), dim=1)\n",
    "        prediction[:, 0, :, :] += 1.\n",
    "        prediction[:, 0, :, :] *= 50.\n",
    "        prediction[:, 1, :, :] *= 110.\n",
    "        prediction[:, 2, :, :] *= 110.\n",
    "        pred_arr = prediction.numpy()\n",
    "        \n",
    "        from skimage import color\n",
    "        rgb_image = np.transpose(pred_arr, (0, 2, 3, 1))\n",
    "        rgb_image = color.lab2rgb(rgb_image[0])\n",
    "        rgb_image = np.clip(rgb_image, 0, 1) * 255\n",
    "        rgb_image = rgb_image.astype(np.uint8)\n",
    "        image_pil = Image.fromarray(rgb_image)\n",
    "            \n",
    "        self.train()\n",
    "        return image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a033fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0002\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dl, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 25\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrunning_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_dl, epochs, learning_rate=0.001):\n",
    "    criterion = nn.MSELoss()  # Funkcja straty Mean Squared Error\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_idx, data in tqdm(enumerate(train_dl), total=len(train_dl)):\n",
    "            inputs, targets = data['L'], data['ab']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dl)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "model = UNet().to('cpu')\n",
    "train_model(model, train_dl, epochs=100, learning_rate=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a530dbb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './trained_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet()\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./trained_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './trained_model.pth'"
     ]
    }
   ],
   "source": [
    "model = UNet()\n",
    "model.load_state_dict(torch.load('./trained_model.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "643054f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]             640\n",
      "            Conv2d-2         [-1, 64, 256, 256]          36,928\n",
      "        Conv_block-3         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-4         [-1, 64, 128, 128]               0\n",
      "     Encoder_block-5  [[-1, 64, 256, 256], [-1, 64, 128, 128]]               0\n",
      "            Conv2d-6        [-1, 128, 128, 128]          73,856\n",
      "            Conv2d-7        [-1, 128, 128, 128]         147,584\n",
      "        Conv_block-8        [-1, 128, 128, 128]               0\n",
      "         MaxPool2d-9          [-1, 128, 64, 64]               0\n",
      "    Encoder_block-10  [[-1, 128, 128, 128], [-1, 128, 64, 64]]               0\n",
      "           Conv2d-11          [-1, 256, 64, 64]         295,168\n",
      "           Conv2d-12          [-1, 256, 64, 64]         590,080\n",
      "       Conv_block-13          [-1, 256, 64, 64]               0\n",
      "        MaxPool2d-14          [-1, 256, 32, 32]               0\n",
      "    Encoder_block-15  [[-1, 256, 64, 64], [-1, 256, 32, 32]]               0\n",
      "           Conv2d-16          [-1, 512, 32, 32]       1,180,160\n",
      "           Conv2d-17          [-1, 512, 32, 32]       2,359,808\n",
      "       Conv_block-18          [-1, 512, 32, 32]               0\n",
      "        MaxPool2d-19          [-1, 512, 16, 16]               0\n",
      "    Encoder_block-20  [[-1, 512, 32, 32], [-1, 512, 16, 16]]               0\n",
      "           Conv2d-21         [-1, 1024, 16, 16]       4,719,616\n",
      "           Conv2d-22         [-1, 1024, 16, 16]       9,438,208\n",
      "       Conv_block-23         [-1, 1024, 16, 16]               0\n",
      "  ConvTranspose2d-24         [-1, 1024, 32, 32]       4,195,328\n",
      "           Conv2d-25          [-1, 512, 32, 32]       7,078,400\n",
      "           Conv2d-26          [-1, 512, 32, 32]       2,359,808\n",
      "    Decoder_block-27          [-1, 512, 32, 32]               0\n",
      "  ConvTranspose2d-28          [-1, 512, 64, 64]       1,049,088\n",
      "           Conv2d-29          [-1, 256, 64, 64]       1,769,728\n",
      "           Conv2d-30          [-1, 256, 64, 64]         590,080\n",
      "    Decoder_block-31          [-1, 256, 64, 64]               0\n",
      "  ConvTranspose2d-32        [-1, 256, 128, 128]         262,400\n",
      "           Conv2d-33        [-1, 128, 128, 128]         442,496\n",
      "           Conv2d-34        [-1, 128, 128, 128]         147,584\n",
      "    Decoder_block-35        [-1, 128, 128, 128]               0\n",
      "  ConvTranspose2d-36        [-1, 128, 256, 256]          65,664\n",
      "           Conv2d-37         [-1, 64, 256, 256]         110,656\n",
      "           Conv2d-38         [-1, 64, 256, 256]          36,928\n",
      "    Decoder_block-39         [-1, 64, 256, 256]               0\n",
      "           Conv2d-40          [-1, 2, 256, 256]             130\n",
      "================================================================\n",
      "Total params: 36,950,338\n",
      "Trainable params: 36,950,338\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 44563978.00\n",
      "Params size (MB): 140.95\n",
      "Estimated Total Size (MB): 44564119.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1011ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_check(validiation_set):\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    from tqdm.notebook import tqdm\n",
    "    ssim_list = list()\n",
    "    for elements in tqdm(val_set):\n",
    "        pic = glob.glob(elements)\n",
    "        img_test = make_dataloaders(paths=pic, batch_size=1)\n",
    "        channel = next(iter(img_test))\n",
    "\n",
    "        Ls, abs_ = channel['L'], channel['ab']\n",
    "        \n",
    "        pred = model.predict(Ls)\n",
    "    \n",
    "        image_ref = Image.open(elements)\n",
    "        image_ref = image_ref.resize((256, 256), Image.BICUBIC)\n",
    "        image_ref = np.array(image_ref)\n",
    "        \n",
    "        image_pred = np.array(pred)\n",
    "        \n",
    "        ssim_val = ssim(image_ref, image_pred, win_size=3, multichannel=True)\n",
    "        ssim_list.append(ssim_val)       \n",
    "        \n",
    "    return ssim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53e5b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: UNet, image):\n",
    "    '''\n",
    "    Image color prediction\n",
    "    '''\n",
    "\n",
    "    img_normalized = image / 50.0 - 1\n",
    "    img_tensor = torch.tensor(img_normalized).float().unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    image_pred = model.predict(img_tensor)\n",
    "    \n",
    "    return image_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e50696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
